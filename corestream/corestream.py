import copy
import hdbscan
import math
import os
import typing
import time
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import networkx as nx
import torch.nn as nn
import sys

from river import base
from collections import defaultdict, deque
from sklearn.cluster import AgglomerativeClustering
from .data_bubble import Vertex, DataBubble
from .coressg import CoreSSG
from .updating import Updating
from .dendrogram import Dendrogram
from .evaluation import Evaluation
from .minimal_spaning_tree import MinimalSpaningTree
from .mutual_reachability_graph import MutualReachabilityGraph
# parallelism
from multiprocessing import Pool, cpu_count

class CoreStream(base.Clusterer, nn.Module):

    class BufferItem:
        def __init__(self, x, timestamp, covered):
            self.x = x
            self.timestamp = (timestamp,)
            self.covered   = covered
    
    def __init__(
        self,
        mpts                   = [10],
        min_cluster_size       = 10,
        decaying_factor: float = 0.25,
        beta:            float = 0.75,
        mu:              float = 2,
        epsilon:         float = 0.02,
        n_samples_init:  int   = 2000,
        stream_speed:    int   = 100,
        percent                = 0.10,
        method_summarization   = 'single_linkage',
        runtime                = False,
        plot                   = False,
        save_partitions        = False,
        dataset                = ''
    ):
        super().__init__()
        self.percent              = percent
        self.timestamp            = 0
        self.initialized          = False
        self.decaying_factor      = decaying_factor
        self.beta                 = beta
        self.mu                   = mu
        self.epsilon              = epsilon
        self.n_samples_init       = n_samples_init
        self.stream_speed         = stream_speed
        self.mst                  = None
        self.mpts                 = mpts
        self.min_cluster_size     = min_cluster_size
        self.method_summarization = method_summarization
        self.runtime              = runtime
        self.plot                 = plot
        self.save_partitions      = save_partitions
        self.base_dir_result      = os.path.join(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')), "results/" + str(dataset) + "/")
        self.dataset              = dataset

        # number of clusters generated by applying the variant of DBSCAN algorithm
        # on p-micro-cluster centers and their centers
        self.n_clusters = 0
        
        self.clusters: typing.Dict[int, "DataBubble"]       = {}
        self.p_data_bubbles: typing.Dict[int, "DataBubble"] = {}
        self.o_data_bubbles: typing.Dict[int, "DataBubble"] = {}
        
        self._n_samples_seen = 0
        self.m_update        = None
        
        self._time_period = math.ceil((1 / self.decaying_factor) * math.log((self.mu * self.beta) / (self.mu * self.beta - 1))) + 1
        print("Time period: ", self._time_period)
        
        if self.method_summarization == 'epsilon':
            self._init_buffer: typing.Deque[typing.Dict] = deque()
        else:
            self._init_buffer = []
        
        # DataFrame to save the runtimes
        if self.runtime:
            self.df_runtime_final = pd.DataFrame(columns=['timestamp', 'data_bubbles', 'summarization', 'mrg', 'mst', 'core_sg', 'multiple_hierarchies'])

        if self.save_partitions:
            # DataFrame to save summarized objects from Data Bubbles
            self.df_bubbles_to_points = pd.DataFrame({
                "0": pd.Series(dtype="float"),
                "1": pd.Series(dtype="float"),
                "id_db": pd.Series(dtype="Int64")
            })
        
        # check that the value of beta is within the range (0,1]
        if not (0 < self.beta <= 1):
            raise ValueError(f"The value of `beta` (currently {self.beta}) must be within the range (0,1].")

    @property
    def centers(self):
        return {k: cluster.getRep(self.timestamp) for k, cluster in self.clusters.items()}

    @staticmethod
    def _distance(point_a, point_b):
        square_sum = 0
        dim        = len(point_a)
        
        for i in range(dim):
            square_sum += math.pow(point_a[i] - point_b[i], 2)
        
        return math.sqrt(square_sum)

    def _get_closest_cluster_key(self, point, clusters):
        min_distance = math.inf
        key          = -1
        
        for k, cluster in clusters.items():
            distance = self.distanceEuclidian(cluster.getRep(self.timestamp), point)
            
            if distance < min_distance and distance <= self.epsilon:
                min_distance = distance
                key          = k
                
        return key

    def distanceEuclidian(self, x1, x2):
        distance = 0
        
        for i in range(len(x1)):
            d         = x1[i] - x2[i]
            distance += d * d
            
        return math.sqrt(distance)
    
    # Merge from CoreStream
    def _merge(self, point):
        # initiate merged status
        merged_status = False

        pos = self._n_samples_seen - 1
        
        if self.save_partitions:
            self.df_bubbles_to_points.loc[pos, '0']     = point[0]
            self.df_bubbles_to_points.loc[pos, '1']     = point[1]
            self.df_bubbles_to_points.loc[pos, 'id_db'] = 0

        if len(self.p_data_bubbles) != 0:
            # try to merge p into its nearest p-data_bubble c_p
            closest_pdb_key = self._get_closest_cluster_key(point, self.p_data_bubbles)
            
            if closest_pdb_key != -1:
                updated_pdb = copy.deepcopy(self.p_data_bubbles[closest_pdb_key])
                updated_pdb.insert(point, self.timestamp)
                
                if updated_pdb.getExtent(self.timestamp) <= self.epsilon:
                    # keep updated p-data_bubble
                    self.p_data_bubbles[closest_pdb_key] = updated_pdb

                    if self.save_partitions:
                        self.df_bubbles_to_points.at[pos, 'id_db'] = closest_pdb_key

                    merged_status = True

        if not merged_status:
            closest_odb_key = self._get_closest_cluster_key(point, self.o_data_bubbles)
            
            if closest_odb_key != -1:
                updated_odb = copy.deepcopy(self.o_data_bubbles[closest_odb_key])
                updated_odb.insert(point, self.timestamp)

                if updated_odb.getExtent(self.timestamp) <= self.epsilon:
                    # keep updated o-micro-cluster
                    weight_odb = updated_odb._weight(self.timestamp)
                    
                    if weight_odb > self.mu * self.beta:
                        # it has grown into a p-micro-cluster
                        del self.o_data_bubbles[closest_odb_key]

                        new_key = 0
                        
                        while new_key in self.p_data_bubbles:
                            new_key += 1
                            
                        updated_odb.setID(new_key)
                        self.p_data_bubbles[new_key] = updated_odb
                        
                        if self.save_partitions:
                            self.df_bubbles_to_points.at[pos, 'id_db'] = new_key
                            self.df_bubbles_to_points['id_db']         = self.df_bubbles_to_points['id_db'].replace((-1) * closest_odb_key, new_key)
                            
                    else:
                        self.o_data_bubbles[closest_odb_key] = updated_odb

                        if self.save_partitions:
                            # Outliers have our key negative
                            self.df_bubbles_to_points.at[pos, 'id_db'] = (-1) * closest_odb_key
                    
                    merged_status = True
                    
        if not merged_status:
            # create a new o-data_bubble by p and add it to o_data_bubbles
            db_from_p = DataBubble(x=point, timestamp=self.timestamp, decaying_factor=self.decaying_factor)

            key_o = 2

            while key_o in self.o_data_bubbles:
                key_o += 1

            self.o_data_bubbles[key_o] = db_from_p

            if self.save_partitions:
                self.df_bubbles_to_points.at[pos, 'id_db'] = (-1) * key_o

            merged_status = True

    def _is_directly_density_reachable(self, c_p, c_q):
        if c_p._weight(self.timestamp) > self.mu and c_q._weight(self.timestamp) > self.mu:
            # check distance of two clusters and compare with 2*epsilon
            c_p_center = c_p.getRep()
            c_q_center = c_q.getRep()
            distance   = self._distance(c_p_center, c_q_center)
            
            if distance < 2 * self.epsilon and distance <= c_p.calc_radius() + c_q.calc_radius():
                return True
            
        return False

    def _query_neighbor(self, cluster):
        neighbors = deque()
        # scan all clusters within self.p_data_bubbles
        for pmc in self.p_data_bubbles.values():
            # check density reachable and that the cluster itself does not appear in neighbors
            if cluster != pmc and self._is_directly_density_reachable(cluster, pmc):
                neighbors.append(pmc)
        return neighbors

    @staticmethod
    def _generate_clusters_for_labels(cluster_labels):
        # initiate the dictionary for final clusters
        clusters = {}

        # group clusters per label
        mcs_per_label = defaultdict(deque)
        
        for mc, label in cluster_labels.items():
            mcs_per_label[label].append(mc)

        # generate set of clusters with the same label
        for label, micro_clusters in mcs_per_label.items():
            # merge clusters with the same label into a big cluster
            cluster = copy.copy(micro_clusters[0])
            
            for mc in range(1, len(micro_clusters)):
                cluster.merge(micro_clusters[mc])

            clusters[label] = cluster

        return len(clusters), clusters       

    def _build(self):

        print("\n>> Timestamp: ", self.timestamp)

        self.time_period_check()
        
        if self.runtime:
            start_coresg = time.time()
        
        print("> count_potential: ", len(self.p_data_bubbles))
        print("> count_outlier: ", len(self.o_data_bubbles))
        
        # (self.mpts / self.mu) is the wort case, when all data bubbles have the slef.mu points
        if len(self.p_data_bubbles) < (max(self.mpts) / self.mu):
            print("no building possible since num_potential_dbs < Mpts")
            return
        
        Vertex.s_idCounter = 0
        
        for db in self.p_data_bubbles.values():
            db.setVertexRepresentative(None)
            db.setStaticCenter(self.timestamp)
        
        G = nx.Graph()
        
        # MRG --
        start_mrg = time.time()
        mrg       = MutualReachabilityGraph(G, self.p_data_bubbles.values(), max(self.mpts), self.timestamp)
        mrg.buildGraph()
        end_mrg   = time.time()
        
        print("> Time for MRG: ", end_mrg - start_mrg)
        # ------
        
        knng = mrg.getKnngGraph()
        
        # MST --
        start_mst = time.time()
        T         = nx.minimum_spanning_tree(G)
        mst_max   = MinimalSpaningTree(T)
        mst_max.buildGraph()
        end_mst   = time.time()
        
        print("> Time for MST:", end_mst - start_mst)
        # ------
        
        csg = CoreSSG(mst_max, knng, self.timestamp)
        end_coresg = time.time()
        
        self.m_update = Updating(mrg, mst_max, csg)
        self.mst      = mst_max
        
        # MULTIPLE HIERARCHIES -------------------------------------------------------------------------------------------
        if self.save_partitions:
            self.remove_oldest_points_in_bubbles_timestamp()
            self.data_bubbles_to_points(self.timestamp)
        
        print("Computing Multiple Hierarchies:")
        start_hierarchies = time.time()
        # PARALLELISM
        try: 
            args = [(csg, mptsi) for mptsi in self.mpts]

            with Pool(processes = (cpu_count() - 5)) as pool:
                results = pool.starmap(self.compute_hierarchy_mpts, args)
        except KeyboardInterrupt:
            print("Interrompido pelo usuÃ¡rio")

        end_hierarchies = time.time()
        print("> Time for Multiple Hierarchies: ", end_hierarchies - start_hierarchies)

        if self.save_partition:
            # ASSESSMENT
            evaluation = Evaluation(self.dataset, self.mpts, self.timestamp)
            evaluation.evaluation_mensure()

        if self.runtime:
            self.save_runtime_timestamps(results)
            
            # Time Final Timestamps
            self.df_runtime_final.at[self.timestamp, 'timestamp']            = self.timestamp
            self.df_runtime_final.at[self.timestamp, 'data_bubbles']         = len(self.p_data_bubbles)
            self.df_runtime_final.at[self.timestamp, 'mrg']                  = end_mrg - start_mrg
            self.df_runtime_final.at[self.timestamp, 'mst']                  = end_mst - start_mst
            self.df_runtime_final.at[self.timestamp, 'core_sg']              = (end_coresg - start_coresg)
            self.df_runtime_final.at[self.timestamp, 'multiple_hierarchies'] = end_hierarchies - start_hierarchies
        
        print("\n****************************************************************************************")
        
        # Plot MRG      -> mrg.buildAbsGraph()
        # Plot KNNG     -> knng.buildAbsGraph(self.timestamp)
        # Plot MST Max  -> mst_max.buildAbsGraph(self.timestamp)
        # GraphViz MRG  -> mrg.getGraphVizString()
        # GraphViz MST  -> mst_max.getGraphVizString()
        # GraphViz KNNG -> knng.getGraphVizString()
        # GraphViz CSG  -> csg.getGraphVizString()

    def compute_hierarchy_mpts(self, csg, mptsi):

        if self.save_partitions:
            df_partition = self.df_bubbles_to_points[(self.df_bubbles_to_points['id_db'] != -1)]
            len_points   = df_partition.shape[0]
            len_dbs      = len(csg.getVertices())
        
            # OBJECTS PARTITION
            partition_bubbles_o = [-1 for j in range(len_points)]
            partition_hdbscan_o = [-1 for j in range(len_points)]
 
        start_time_total = time.time()
        
        print("\n-------------------------------------------------------------------------------------")
        print("Mpts: " + str(mptsi))
        
        if mptsi != max(self.mpts):
            start_hierarchy = time.time()
            csg.computeHierarchyMpts(mptsi)
            end_hierarchy   = time.time()

            print("> Time for CORE-SG edges: ", end_hierarchy - start_hierarchy)

            # MST
            start_mst   = time.time()
            G           = csg.getGraphNetworkx()
            T           = nx.minimum_spanning_tree(G, weight='weight')
            mst_csg     = MinimalSpaningTree(T)
            mst_csg.buildGraph()
            end_mst     = time.time()

            print("> Time for MST: ", end_mst - start_mst)

            self.m_update = Updating(None, mst_csg, csg)
        
        end_time_total = time.time()
        print("> Total Time MPts: ", end_time_total - start_time_total)
        
        start_selection = end_selection = 0
        
        # Time Selection Clusters
        if self.save_partitions:
            # Dendrogram
            start_dendrogram = time.time()
            dendrogram       = Dendrogram(self.m_update.getMST(), mptsi, mptsi, self.timestamp) # MST, miClusterSize, mptsi
            dendrogram.build()
            end_dendrogram   = time.time()
            print("> Total Time Dendrogram: ", end_dendrogram - start_dendrogram)

            start_selection = time.time()
            selection       = dendrogram.clusterSelection()
            partition       = [-1 for j in range(len_dbs + 10)]

            # Partitions Corestream Mpts
            cont             = 1
            partition_bubble = {}
        
            # setar o valor do cluster nos DBs
            for n in selection:
                it = iter(n.getVertices())

                for el in it:
                    partition_bubble[el.getDataBubble().getID()] = cont
                    partition[el.getID()]                        = cont

                cont += 1

            #setar todos os objetos sendo o cluster do DB que ao qual pertence
            cont = 0
            for i, row in df_partition.iterrows():
                if row['id_db'] in partition_bubble:
                    partition_bubbles_o[cont] = partition_bubble[row['id_db']]
                cont += 1
        
            end_selection = time.time()
            
            self.save_partition(partition, csg.getVertices(), mptsi)

            # Partitions HDBSCAN
            start_hdbscan = time.time()
            
            clusterer = hdbscan.HDBSCAN(min_cluster_size = 10, min_samples = mptsi, match_reference_implementation = True, core_dist_n_jobs = 1)
            clusterer.fit(df_partition.drop("id_db", axis=1))
            labels    = clusterer.labels_
        
            p = 0
            for i in labels:
                partition_hdbscan_o[p] = i
                p += 1
            
            end_hdbscan = time.time()

            print(">Time for HDBSCAN: ", end_hdbscan - start_hdbscan)

            # PLOT
            if self.plot:
                self.plot_partition(partition, mptsi, df_partition)
                self.plot_hdbscan_result(mptsi, labels, df_partition)

            # SAVING OBJECTS PARTITIONS
            self.save_partition_bubble_and_objects_mpts(partition_bubbles_o, partition_hdbscan_o, mptsi)
        
        # Runtime
        if self.runtime:
            df_runtime_timestamps = {}
            df_runtime_timestamps['mpts'] = mptsi
            
            if mptsi != max(self.mpts):
                df_runtime_timestamps['core_sg'] = (end_hierarchy - start_hierarchy)
                df_runtime_timestamps['mst']     = (end_mst - start_mst)
            else:
                df_runtime_timestamps['core_sg'] = 0
                df_runtime_timestamps['mst']     = 0
            
            df_runtime_timestamps['dendrogram'] = (end_dendrogram - start_dendrogram)
            df_runtime_timestamps['selection']  = (end_selection - start_selection)
            df_runtime_timestamps['total']      = (end_time_total - start_time_total)

            return df_runtime_timestamps

        # GraphViz CORE-SG    -> csg.getGraphVizString(self.timestamp, i)
        # GraphViz MST        -> mst_csg.getGraphVizString(self.timestamp, i)
        # Plot MST            -> mst_csg.buildAbsGraph(self.timestamp)
        # GraphViz Dendrogram -> if i == 200: dendrogram.getGraphVizString()
    
    def _expand_cluster(self, db, neighborhood):
        for idx in neighborhood:
            item = self._init_buffer[idx]
            
            if not item.covered:
                item.covered = True
                db.insert(item.x, self.timestamp)

    def _get_neighborhood_ids(self, item):
        neighborhood_ids = deque()
        
        for idx, other in enumerate(self._init_buffer):
            if not other.covered:
                #print(">> ", self._distance(item.x, other.x))
                if self._distance(item.x, other.x) < self.epsilon:
                    neighborhood_ids.append(idx)
        
        return neighborhood_ids
    
    def _initial_epsilon(self):
        start = time.time()
        
        for item in self._init_buffer:
            if not item.covered:
                item.covered = True
                neighborhood = self._get_neighborhood_ids(item)
                
                if len(neighborhood) > self.mu:
                    db = DataBubble(
                        x=item.x,
                        timestamp=self.timestamp,
                        decaying_factor=self.decaying_factor,
                    )
                    self._expand_cluster(db, neighborhood)
                    db.setStaticCenter(self.timestamp)
                    self.p_data_bubbles.update({len(self.p_data_bubbles): db})
                else:
                    item.covered = False
                    
        end = time.time()
        
        if self.runtime:
            self.df_runtime_final.at[self.timestamp, 'summarization'] = end - start
        
        print("> Time for Summarization: ", end - start)
    
    def _initial_single_linkage(self):
        start = time.time()
        
        self._init_buffer = np.array(self._init_buffer)
        
        # The linkage="single" does a clustering, e. g., the clusters are indentified and form big data bubbles.
        clustering = AgglomerativeClustering(n_clusters = int(self.n_samples_init * self.percent), linkage='average')
        clustering.fit(self._init_buffer)
        
        labels          = clustering.labels_
        labels_visited  = np.zeros(len(labels))
        len_buffer      = len(self._init_buffer)
        count_potential = 0
        min_db = max_db = 0
        epsilon         = {}
        pos_point       = 0

        if self.save_partition:
            bubbles_to_points = []
        
        for i in range(len_buffer):
            
            label     = labels[i]
            object_new = dict(enumerate(self._init_buffer[i]))
            labels_visited[label] += 1
            
            if labels_visited[label] == 1:
                db = DataBubble(
                    x = object_new,
                    timestamp = self.timestamp,
                    decaying_factor = self.decaying_factor,
                )

                db.setID(label)
                
                self.p_data_bubbles.update({label: db})
                
            else:
                self.p_data_bubbles[label].insert(object_new, self.timestamp)

                if labels_visited[label] == self.mu:
                    count_potential += 1
                if self.p_data_bubbles[label].getN() >= self.mu:
                    epsilon[label] = self.p_data_bubbles[label].getExtent(self.timestamp)
                
            max_db = max(max_db, self.p_data_bubbles[label].getN())
    
            if self.save_partitions:
                bubbles_to_points.append([self._init_buffer[i][0], self._init_buffer[i][1], label])
                pos_point += 1

        if self.save_partitions:
            self.df_bubbles_to_points = pd.DataFrame(bubbles_to_points, columns=['0', '1', 'id_db'])
        
        # outliers data_bubbles
        if count_potential != len(self.p_data_bubbles):
            replace_map = {}
            key         = 0
            key_p       = 0
            key_o       = 2
            
            while labels_visited[key]:
                if labels_visited[key] < self.mu:
                    if self.save_partitions:
                        replace_map[key] = (-1) * key_o
                    
                    self.o_data_bubbles[key_o] = self.p_data_bubbles[key]
                    self.p_data_bubbles.pop(key)

                    key_o += 1
                else:
                    if key != key_p:
                        self.p_data_bubbles[key].setID(key_p)
                        self.p_data_bubbles[key_p] = self.p_data_bubbles.pop(key)

                        if min_db == 0:
                            min_db = self.p_data_bubbles[key_p].getN()
                        else:
                            min_db = min(min_db, self.p_data_bubbles[key_p].getN())
                        
                        if self.save_partitions:
                            #update new key
                            replace_map[key] = key_p

                    key_p += 1
                key += 1

            if self.save_partitions:
                self.df_bubbles_to_points['id_db'] = self.df_bubbles_to_points['id_db'].replace(replace_map)
                
        end = time.time()
        
        # Time
        if self.runtime:
            self.df_runtime_final.at[self.timestamp, 'summarization'] = end - start
        
        e_min  = min(epsilon.values())
        e_mean = sum(epsilon.values()) / count_potential
        e_max  = max(epsilon.values())
        
        self.epsilon = e_max
        
        print("> Total: ", (count_potential + len(self.o_data_bubbles)))
        print("> Bubbles Potential: ", count_potential)
        print("> Min_DB: ", min_db)
        print("> Max_DB: ", max_db)
        print("> Time for DBs: ", end - start)
        print("> Epsilon min: ", e_min)
        print("> Epsilon mean: ", e_mean)
        print("> Epsilon max: ", e_max)

    def time_period_check(self):
        # Periodic cluster removal
            
        for i, p_data_bubble_i in list(self.p_data_bubbles.items()):
            if p_data_bubble_i._weight(self.timestamp) < self.mu * self.beta:
                # c_p became an outlier and should be deleted
                self.p_data_bubbles.pop(i)

        for j, o_data_bubble_j in list(self.o_data_bubbles.items()):
            # calculate xi
            xi = (2**(-self.decaying_factor * (self.timestamp - o_data_bubble_j.creation_time + self._time_period)) - 1) / (2 ** (-self.decaying_factor * self._time_period) - 1)

            if o_data_bubble_j._weight(self.timestamp) < xi:
                # c_o might not grow into a p-micro-cluster, we can safely delete it
                self.o_data_bubbles.pop(j)
        
    def learn_one(self, x, sample_weight=None):
        self._n_samples_seen += 1
        # control the stream speed
        
        if self._n_samples_seen % self.stream_speed == 0:
            self.timestamp += 1

        # Initialization
        if not self.initialized:
            if self.method_summarization == 'epsilon':
                self._init_buffer.append(self.BufferItem(x, self.timestamp, False))
            else:
                self._init_buffer.append(list(x.values()))
            
            
            if len(self._init_buffer) == self.n_samples_init:
                print("entrando no initial()")
                if self.method_summarization == 'epsilon':
                    self._initial_epsilon()
                else:
                    self._initial_single_linkage()
                
                self.initialized = True
                
                del self._init_buffer
                
            return self

        # Merge
        self._merge(x)

        if self.timestamp > 0 and self.timestamp % self._time_period == 0:
            self.time_period_check()
        
        return self

    def predict_one(self, sample_weight=None):        
        # This function handles the case when a clustering request arrives.
        # implementation of the DBSCAN algorithm proposed by Ester et al.
        
        if not self.initialized:
            # The model is not ready
            return 0
        
        self._build()
    
    def save_partition(self, partition, vertices, mpts):
        m_directory = os.path.join(self.base_dir_result, "flat_solutions")
        
        try:
            sub_dir = os.path.join(m_directory, "flat_solution_partitions_t" + str(self.timestamp) + "/partitions_bubbles")

            if not os.path.exists(sub_dir):
                os.makedirs(sub_dir)
            
            # SAVE MPTS PARTITION
            df_partition                   = pd.DataFrame([partition])
            df_partition['partition_mpts'] = mpts
            df_partition.columns           = df_partition.columns.map(str)
            df_partition.to_parquet(os.path.join(sub_dir, f"partitions_mpts_{mpts}.parquet"), index=True)

            if self.plot:
                cores = ["blue", "red", "orange", "green", "purple", "brown", "pink", "olive", "cyan"]

                with open(os.path.join(sub_dir, "partition_plot_mpts_" + str(mpts) + ".csv"), 'w') as writer:
                    writer.write("x,y,N,radio,color,cluster,ID\n")

                    for v in vertices:
                        if partition[v.getID()] == -1:
                            writer.write(str(v.getDataBubble().getRep(self.timestamp)[0]) + "," + str(v.getDataBubble().getRep(self.timestamp)[1]) + "," + str(v.getDataBubble()._weight(self.timestamp)) + "," + str(v.getDataBubble().getExtent(self.timestamp)) + ",black,-1" + "," + str(v.getDataBubble().getID()) + "\n")
                        else:
                            writer.write(str(v.getDataBubble().getRep(self.timestamp)[0]) + "," + str(v.getDataBubble().getRep(self.timestamp)[1]) + "," + str(v.getDataBubble()._weight(self.timestamp)) + "," + str(v.getDataBubble().getExtent(self.timestamp)) + "," + cores[partition[v.getID()] % 9] + "," + str(partition[v.getID()]) + "," + str(v.getDataBubble().getID()) + "\n")
        
        except FileNotFoundError as e:
            print(e)
    
    def plot_partition(self, partition, mpts, df_partition):
        sns.set_context('poster')
        sns.set_style('white')
        sns.set_color_codes()
        
        plot_kwds = {'s' : 3, 'linewidths':0}
        
        m_directory = os.path.join(self.base_dir_result, "plots")
        
        try:
            sub_dir = os.path.join(m_directory, "plot_bubbles_t" + str(self.timestamp))

            if not os.path.exists(sub_dir):
                os.makedirs(sub_dir)
        
            partition = pd.read_csv(str(self.base_dir_result) + 'flat_solutions/flat_solution_partitions_t' + str(self.timestamp) + '/partitions_bubbles/partition_plot_mpts_' + str(mpts) + '.csv', sep=',')

            # Statistic partition-------------------------------
            count_outlier = 0
            count_cluster = 0

            for j in range(len(partition)):

                if(partition['cluster'].loc[j] == -1):
                    count_outlier += 1

                if(partition['cluster'].loc[j] > count_cluster):
                    count_cluster = partition['cluster'].loc[j]

            legend  = ""
            legend += "Mpts: " + str(mpts) + "  "
            legend += "| Outliers: " + str(int((count_outlier * 100.0) / len(partition))) + "%  "
            legend += "| Clusters: " + str(count_cluster) + "  "
            legend += "| DBs: " + str(len(partition)) + "  "
            legend += "| Timestamp: " + str(self.timestamp)
            # -------------------------------------------------

            plt.figure(figsize = (16,12))

            for j in range(len(partition)):
                #plt.plot([partition['x'].loc[j]], [partition['y'].loc[j]], 'o', ms=partition['radio'].loc[j] * 50, mec=partition['color'].loc[j], mfc='none', mew=2)
                plt.gca().add_patch(plt.Circle((partition['x'].loc[j], partition['y'].loc[j]), partition['radio'].loc[j], color=partition['color'].loc[j], fill=False))
                #plt.text(partition['x'].loc[j], partition['y'].loc[j], str(partition['ID'].loc[j]), fontsize=10, ha='center', va='center')

            plt.scatter(df_partition['0'], df_partition['1'], **plot_kwds, label=legend)
            plt.legend(bbox_to_anchor=(-0.1, 1.02, 1, 0.2), loc="lower left", borderaxespad=0, fontsize=28)
            plt.savefig(str(sub_dir) + "/mpts_" + str(mpts) + ".png")
            plt.close('all')

        except FileNotFoundError as e:
            print(e)

    def plot_hdbscan_result(self, mpts, labels, df_partition):
        m_directory = os.path.join(self.base_dir_result, "plots")
        sub_dir     = os.path.join(m_directory, "plot_bubbles_t" + str(self.timestamp))

        if not os.path.exists(sub_dir):
            os.makedirs(sub_dir)
        
        sns.set_context('poster')
        sns.set_style('white')
        sns.set_color_codes()
        
        plot_kwds = {'s' : 10, 'linewidths':0}

        plt.figure(figsize = (16,12))
        title  = ""
        title += "HDBSCAN Mpts: " + str(mpts) + " | "
        title += "Clusters: " + str(len(set(labels)) - (1 if -1 in labels else 0)) + " | "
        title += "Outliers: " + str(np.sum(labels == -1)) + " | "
        title += "Len Objects: " + str(len(labels))

        plt.title(title)
        plt.scatter(df_partition['0'], df_partition['1'], c=labels, cmap='magma', **plot_kwds)
        plt.savefig(str(self.base_dir_result) + "plots/plot_bubbles_t" + str(self.timestamp) + "/mpts _" + str(mpts) + "_hdbscan.png")
        plt.close('all')
    
    def save_partition_bubble_and_objects_mpts(self, partition_bubbles_o, partition_hdbscan_o, mpts):

        m_directory = os.path.join(self.base_dir_result, "flat_solutions")
        
        try:
            sub_dir = os.path.join(m_directory, "flat_solution_partitions_t" + str(self.timestamp) + "/partitions_objects")

            if not os.path.exists(sub_dir):
                os.makedirs(sub_dir)

            df_partition_bubbles_o                   = pd.DataFrame([partition_bubbles_o])
            df_partition_bubbles_o['partition_mpts'] = mpts
            df_partition_bubbles_o.columns           = df_partition_bubbles_o.columns.map(str)
            df_partition_bubbles_o.to_parquet(os.path.join(sub_dir, f"partition_bubbles_mpts_{mpts}.parquet"), index=True)

            df_partitons_hdbscan_o                   = pd.DataFrame([partition_hdbscan_o])
            df_partitons_hdbscan_o['partition_mpts'] = mpts
            df_partitons_hdbscan_o.columns           = df_partitons_hdbscan_o.columns.map(str)
            df_partitons_hdbscan_o.to_parquet(os.path.join(sub_dir, f"partition_hdbscan_mpts_{mpts}.parquet"), index=True)

        except FileNotFoundError as e:
            print(e)
    
    def save_runtime_timestamps(self, results):
        m_directory = os.path.join(self.base_dir_result, "runtime")
        
        try:
            if not os.path.exists(m_directory):
                os.makedirs(m_directory)

            df_runtime_timestamps = pd.DataFrame(results)

            with open(os.path.join(m_directory, "runtime_t" + str(self.timestamp) + ".csv"), 'w') as writer:
                writer.write("mpts,core_sg,mst,dendrogram,selection,total\n")
                
                for _, linha in df_runtime_timestamps.iterrows():
                    writer.write(str(linha['mpts']) + ',' + str(linha['core_sg']) + ',' + str(linha['mst']) + ',' + str(linha['dendrogram']) + ',' + str(linha['selection']) + ',' + str(linha['total']) + "\n")

        except FileNotFoundError as e:
            print(e)
            
    def save_runtime_final(self):
        m_directory = os.path.join(self.base_dir_result, "runtime")
        
        try:
            if not os.path.exists(m_directory):
                os.makedirs(m_directory)

            with open(os.path.join(m_directory, "runtime_final_t" + str(self.timestamp) + ".csv"), 'w') as writer:
                writer.write("timestamp,data_bubbles,summarization,mrg,mst,core_sg,multiple_hierarchies\n")

                for _, linha in self.df_runtime_final.iterrows():
                    writer.write(str(linha['timestamp']) + ',' + str(linha['data_bubbles']) + ',' + str(linha['summarization']) + ',' + str(linha['mrg']) + ',' + str(linha['mst']) + ',' + str(linha['core_sg']) + ',' + str(linha['multiple_hierarchies']) + "\n")

        except FileNotFoundError as e:
            print(e)

    def remove_oldest_points_in_bubbles_timestamp(self):
        
        # Remove oldest objects from removed DBs
        for i, row in self.df_bubbles_to_points.iterrows():
            if i <= self._n_samples_seen:
                if row['id_db'] not in self.p_data_bubbles and row['id_db'] > -1:
                    self.df_bubbles_to_points.at[i, 'id_db'] = -1
                elif ((-1) * row['id_db']) not in self.o_data_bubbles and row['id_db'] < -1:
                    self.df_bubbles_to_points.at[i, 'id_db'] = -1
        
        # Remove oldest objects inside of DBs
        max_id         = max(self.p_data_bubbles.keys())
        labels_visited = np.zeros(max_id + 1)
        diff_points    = np.zeros(max_id + 1)
        
        for i in range(self._n_samples_seen):
            id_db = int(self.df_bubbles_to_points.loc[i, 'id_db'])
            
            if id_db > -1:
                if not labels_visited[id_db] and id_db in self.p_data_bubbles:
                    labels_visited[id_db] = self.df_bubbles_to_points[self.df_bubbles_to_points['id_db'] == id_db].shape[0]
                    n = self.p_data_bubbles[id_db]._weight(self.timestamp)
                    
                    diff_points[id_db] = int(labels_visited[id_db] - n)
                    
                if diff_points[id_db]:
                    diff_points[id_db] -= 1
                    self.df_bubbles_to_points.loc[i, 'id_db'] = -1
                    
        del labels_visited
        del diff_points 
        
        return self.df_bubbles_to_points[self.df_bubbles_to_points['id_db'] != -1]
     
    def data_bubbles_to_points(self, timestamp):
        m_directory = os.path.join(self.base_dir_result, "datasets")
        
        try:
            if not os.path.exists(m_directory):
                os.makedirs(m_directory)
            
            df_filtered         = self.df_bubbles_to_points[(self.df_bubbles_to_points['id_db'] != -1)]
            df_filtered.columns = df_filtered.columns.map(str)
            df_filtered.to_parquet(str(m_directory) + '/data_t' + str(self.timestamp) + '.parquet', index=False)
            
            if self.plot:
                sns.set_context('poster')
                sns.set_style('white')
                sns.set_color_codes()

                plot_kwds = {'s' : 1, 'linewidths':0}

                plt.figure(figsize=(12, 10))

                for _, value in self.p_data_bubbles.items():
                    plt.gca().add_patch(plt.Circle((value.getRep(timestamp)[0], value.getRep(timestamp)[1]), value.getExtent(timestamp), color='red', fill=False))

                for _, value in self.o_data_bubbles.items():
                    plt.gca().add_patch(plt.Circle((value.getRep(timestamp)[0], value.getRep(timestamp)[1]), value.getExtent(timestamp), color='blue', fill=False))

                plt.title("Timestamp: " + str(self.timestamp) + " | # Objects: " + str(df_filtered.shape[0]) + " | # DBs: " + str(len(self.p_data_bubbles)), fontsize=20)
                plt.scatter(df_filtered['0'], df_filtered['1'], c='green', **plot_kwds)
                plt.savefig(str(m_directory) + "/plot_dataset_t" + str(self.timestamp) + ".png")
                plt.close()
            
        except FileNotFoundError as e:
            print(e)